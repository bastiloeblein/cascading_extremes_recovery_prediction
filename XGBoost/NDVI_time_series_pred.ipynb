{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load cubes\n",
    "import os\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "path_dir = \"processed_data_final/\"\n",
    "data_dir = os.listdir(path_dir)\n",
    "print(data_dir)\n",
    "\n",
    "cubes = {}\n",
    "\n",
    "for key in data_dir:\n",
    "    load_path = os.path.join(path_dir, key)\n",
    "    cubes[key] = xr.open_zarr(load_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Neue Strategie: Selecte 2000 pixel pro LC und trainiere auf gesamte zeitreihen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Einstellungen\n",
    "VEGETATION_CLASSES = [10, 20, 30, 40, 60, 90, 95, 100]\n",
    "N_PIXEL_PER_LC = 2000\n",
    "TARGET_VAR = \"NDVI_strict\"\n",
    "\n",
    "def get_feature_list(ds):\n",
    "    \"\"\"Extrahiert alle relevanten Features aus dem Dataset.\"\"\"\n",
    "    all_vars = list(ds.data_vars)\n",
    "    to_remove = [\n",
    "        \"COP_DEM\", \"ESA_LC\", \"NDVI_strict\", \"NDVI_basic\", \"is_veg\", \n",
    "        \"quality_mask_basic\", \"quality_mask_strict\", \"vh_norm\", \"vv_norm\"\n",
    "    ]\n",
    "    return [f for f in all_vars if f not in to_remove]\n",
    "\n",
    "# Feature-Liste initialisieren\n",
    "features = get_feature_list(cubes[\"ds_1\"])\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_df(cubes, train_keys, features, n_pixel_per_lc=2000):\n",
    "    all_cube_samples = []\n",
    "    global_pixel_counter = 0\n",
    "\n",
    "    for key in train_keys:\n",
    "        print(f\"Verarbeite Cube {key}...\")\n",
    "        ds = cubes[key]\n",
    "\n",
    "        # 1. DEM vorbereiten\n",
    "        dem_data = ds[\"COP_DEM\"]\n",
    "        if \"time_cop_dem_glo_30_dged_cog\" in dem_data.dims:\n",
    "            dem_data = dem_data.mean(dim=\"time_cop_dem_glo_30_dged_cog\", skipna=True)\n",
    "\n",
    "        for lc_val in VEGETATION_CLASSES:\n",
    "            # 2. Koordinaten f√ºr diese Klasse finden\n",
    "            coords = np.argwhere((ds.ESA_LC.isel(time_esa_worldcover=0) == lc_val).values)\n",
    "            if len(coords) == 0: continue\n",
    "\n",
    "            n_to_draw = min(len(coords), n_pixel_per_lc)\n",
    "            idx = np.random.choice(len(coords), n_to_draw, replace=False)\n",
    "            selected_coords = coords[idx]\n",
    "\n",
    "            # Vectorized Indexing\n",
    "            y_idx = xr.DataArray(selected_coords[:, 0], dims=\"pixel_id\")\n",
    "            x_idx = xr.DataArray(selected_coords[:, 1], dims=\"pixel_id\")\n",
    "            \n",
    "            pixel_bundle = ds.isel(y=y_idx, x=x_idx)\n",
    "            stacked = pixel_bundle.stack(sample=(\"pixel_id\", \"time_sentinel_2_l2a\"))\n",
    "            n_t = ds.sizes[\"time_sentinel_2_l2a\"]\n",
    "            \n",
    "            # Indizes f√ºr t und t+1 (Vektorisiert)\n",
    "            idx_t = []\n",
    "            idx_target = []\n",
    "            for p in range(n_to_draw):\n",
    "                start = p * n_t\n",
    "                idx_t.extend(range(start, start + n_t - 1))\n",
    "                idx_target.extend(range(start + 1, start + n_t))\n",
    "\n",
    "            # 3. Dictionary bef√ºllen (mit Downcasting auf float32)\n",
    "            data_dict = {\n",
    "                f: stacked[f].isel(sample=idx_t).values.astype(np.float32) for f in features\n",
    "            }\n",
    "\n",
    "            pixel_ids = np.arange(global_pixel_counter, global_pixel_counter + n_to_draw)\n",
    "            \n",
    "            data_dict.update({\n",
    "                \"NDVI_basic_t\": stacked[\"NDVI_basic\"].isel(sample=idx_t).values.astype(np.float32),\n",
    "                \"target_basic\": stacked[\"NDVI_basic\"].isel(sample=idx_target).values.astype(np.float32),\n",
    "                \"NDVI_strict_t\": stacked[\"NDVI_strict\"].isel(sample=idx_t).values.astype(np.float32),\n",
    "                \"target_strict\": stacked[\"NDVI_strict\"].isel(sample=idx_target).values.astype(np.float32),\n",
    "                \"pixel_group\": np.repeat(pixel_ids, n_t - 1),\n",
    "                \"timestep\": np.tile(np.arange(n_t - 1, dtype=np.uint16), n_to_draw),\n",
    "                \"lc_class\": lc_val, # Landcover hinzuf√ºgen\n",
    "                \"cube_origin\": key,\n",
    "                \"COP_DEM\": np.repeat(dem_data.isel(y=y_idx, x=x_idx).values.astype(np.float32), n_t - 1)\n",
    "            })\n",
    "            \n",
    "            global_pixel_counter += n_to_draw\n",
    "            df_t = pd.DataFrame(data_dict)\n",
    "\n",
    "            # 4. Lags (Ged√§chtnis) berechnen\n",
    "            features_to_lag = features + [\"NDVI_basic_t\", \"NDVI_strict_t\"]\n",
    "            for f in features_to_lag:\n",
    "                for lag in range(1, 3):\n",
    "                    df_t[f\"{f}_minus_{lag}\"] = df_t.groupby(\"pixel_group\")[f].shift(lag)\n",
    "\n",
    "            # 5. Cleaning: Wir droppen NUR, wenn basic_target NaN ist\n",
    "            # Da strict aucch immer nan ist wenn basic nan ist - passt das so\n",
    "            df_t = df_t.dropna(subset=[\"target_basic\"])\n",
    "            if not df_t.empty:\n",
    "                all_cube_samples.append(df_t)\n",
    "\n",
    "            del stacked, pixel_bundle\n",
    "            gc.collect()\n",
    "\n",
    "    # Finaler Merge\n",
    "    df_final = pd.concat(all_cube_samples, ignore_index=True)\n",
    "    \n",
    "    # lc_class als kategoriale Variable speichern\n",
    "    df_final[\"lc_class\"] = df_final[\"lc_class\"].astype(\"category\")\n",
    "    \n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define keys\n",
    "test_keys = [\"ds_5\", \"ds_10\"]\n",
    "train_keys = [k for k in cubes.keys() if k not in test_keys]\n",
    "\n",
    "# Anwendung\n",
    "# Strategie: 7 Cubes zum Trainieren, 1 Cube zur Validierung\n",
    "train_keys_subset = train_keys[:-1]  # Die ersten 7\n",
    "val_key_subset = [train_keys[-1]]    # Der 8. Cube nur zur Validierung\n",
    "\n",
    "# 1. Erstelle zwei separate DataFrames\n",
    "df_train_final = create_training_df(cubes, train_keys_subset, features)\n",
    "df_val_final = create_training_df(cubes, val_key_subset, features)\n",
    "print(f\"Fertig! {len(df_train_final)} Zeilen extrahiert.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as parquet\n",
    "df_train_final.to_parquet('XGBoost/new_data/df_train_final_strict_target.parquet', index=False)\n",
    "df_val_final.to_parquet('XGBoost/new_data/df_val_final_strict_target.parquet', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as parquet\n",
    "df_train_final = pd.read_parquet('XGBoost/new_data/df_train_final_strict_target.parquet')\n",
    "df_val_final   = pd.read_parquet('XGBoost/new_data/df_val_final_strict_target.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Create final sets (BASIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Start mit den Basis-Features und statischen Variablen\n",
    "features_basic = features + [\"NDVI_basic_t\", \"COP_DEM\", \"lc_class\"]\n",
    "\n",
    "# 2. F√ºge die Lags f√ºr ALLE Variablen aus der urspr√ºnglichen 'features'-Liste hinzu\n",
    "# Das entspricht der Logik aus deiner create_training_df Loop\n",
    "for f in features:\n",
    "    features_basic += [f\"{f}_minus_{i}\" for i in range(1, 3)]\n",
    "\n",
    "# 3. F√ºge zus√§tzlich die Lags f√ºr das Target (NDVI_basic_t) hinzu\n",
    "features_basic += [f\"NDVI_basic_t_minus_{i}\" for i in range(1, 3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Training Set Basic\n",
    "# X_train_basic = df_train_final[features_basic]\n",
    "# y_train_basic = df_train_final[\"target_basic\"]\n",
    "\n",
    "# # Validation Set Basic\n",
    "# X_val_basic = df_val_final[features_basic]\n",
    "# y_val_basic = df_val_final[\"target_basic\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "df_train_final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## Create final sets (STRICT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Start mit den Basis-Features und statischen Variablen\n",
    "features_strict = features + [\"NDVI_strict_t\", \"COP_DEM\", \"lc_class\"]\n",
    "\n",
    "# 2. F√ºge die Lags f√ºr ALLE Variablen aus der urspr√ºnglichen 'features'-Liste hinzu\n",
    "# Das entspricht der Logik aus deiner create_training_df Loop\n",
    "for f in features:\n",
    "    features_strict += [f\"{f}_minus_{i}\" for i in range(1, 3)]\n",
    "\n",
    "# 3. F√ºge zus√§tzlich die Lags f√ºr das Target (NDVI_basic_t) hinzu\n",
    "features_strict += [f\"NDVI_strict_t_minus_{i}\" for i in range(1,3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Set Strict (Nur Zeilen mit validem strict-Target)\n",
    "df_train_strict = df_train_final.dropna(subset=[\"target_strict\"])\n",
    "# X_train_strict = df_train_strict[features_strict]\n",
    "# y_train_strict = df_train_strict[\"target_strict\"]\n",
    "\n",
    "# # Validation Set Strict\n",
    "# df_val_strict = df_val_final.dropna(subset=[\"target_strict\"])\n",
    "# X_val_strict = df_val_strict[features_strict]\n",
    "# y_val_strict = df_val_strict[\"target_strict\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "# Train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def train_and_save_model(df_train, df_val, features, target_col, model_name, save_path=\"XGBoost/sar_experiments_strict/\"):\n",
    "    \"\"\"\n",
    "    Trainiert ein XGBoost Modell auf einem spezifischen Target (basic oder strict).\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Starte Training f√ºr: {model_name} (Target: {target_col}) ---\")\n",
    "    \n",
    "    # 1. Daten vorbereiten (Cleaning: Nur Zeilen behalten, wo DAS spezifische Target existiert)\n",
    "    # Das ist wichtig f√ºr 'strict', da dort mehr NaNs sind\n",
    "    train_clean = df_train.dropna(subset=[target_col])\n",
    "    val_clean = df_val.dropna(subset=[target_col])\n",
    "    \n",
    "    X_train = train_clean[features]\n",
    "    y_train = train_clean[target_col]\n",
    "    X_val = val_clean[features]\n",
    "    y_val = val_clean[target_col]\n",
    "\n",
    "    # 2. Modell-Initialisierung\n",
    "    model = XGBRegressor(\n",
    "        n_estimators=1000,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=7,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        tree_method=\"hist\",\n",
    "        n_jobs=-1,\n",
    "        early_stopping_rounds=50,\n",
    "        enable_categorical=True\n",
    "    )\n",
    "    \n",
    "    # 3. Fit (Nutzt jetzt die √ºbergebenen Validierungs-Cubes)\n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        verbose=100\n",
    "    )\n",
    "    \n",
    "    # 4. Speichern\n",
    "    model_package = {\n",
    "        \"model\": model,\n",
    "        \"features\": features,\n",
    "        \"model_name\": model_name,\n",
    "        \"target_variable\": target_col\n",
    "    }\n",
    "    \n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    filename = f\"{save_path}{model_name}.pkl\"\n",
    "    joblib.dump(model_package, filename)\n",
    "    print(f\"Modell gespeichert unter: {filename}\")\n",
    "    \n",
    "    # Aufr√§umen\n",
    "    del X_train, X_val, y_train, y_val\n",
    "    gc.collect()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "## New tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Konstante Basis (Klima immer dabei, da du sie sowieso nutzen wirst)\n",
    "climate_vars = ['t2m_mean', 't2mmax_max', 't2mmin_min', 'tp_dailymean_mean', 'tp_dailymax_max', 'tp_rollingmax_max', 'pei_30_mean', 'pei_90_mean']\n",
    "# Klima + Lags\n",
    "climate_core = climate_vars + [f\"{f}_minus_{i}\" for f in climate_vars for i in range(1, 5)]\n",
    "\n",
    "# Statische & NDVI Basis\n",
    "basis = ['COP_DEM', 'lc_class', \"NDVI_strict_t\"] + [f\"NDVI_strict_t_minus_{i}\" for i in range(1, 5)]\n",
    "\n",
    "# Das \"Immer-Dabei\"-Paket\n",
    "full_basis = basis + climate_core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "sar_experiments = [\n",
    "    (\"Full_Basis\", full_basis),\n",
    "    (\"Basis_only\", basis),\n",
    "    (\"Climate_only\", climate_core),\n",
    "    # Einzelsignale (Raw)\n",
    "    (\"VV_only\", full_basis + [\"vv\"] + [f\"vv_minus_{i}\" for i in range(1, 5)]),\n",
    "    (\"VH_only\", full_basis + [\"vh\"] + [f\"vh_minus_{i}\" for i in range(1, 5)]),\n",
    "    \n",
    "    # Kombination der Raw-B√§nder\n",
    "    (\"Dual_Raw\", full_basis + [\"vv\", \"vh\"] + [f\"{f}_minus_{i}\" for f in [\"vv\", \"vh\"] for i in range(1, 5)]),\n",
    "    \n",
    "    # Einzelne Indizes\n",
    "    (\"Index_VHVVR\", full_basis + [\"VHVVR\"] + [f\"VHVVR_minus_{i}\" for i in range(1, 5)]),\n",
    "    (\"Index_VVVHS\", full_basis + [\"VVVHS\"] + [f\"VVVHS_minus_{i}\" for i in range(1, 5)]),\n",
    "    (\"Index_DpRVIVV\", full_basis + [\"DpRVIVV\"] + [f\"DpRVIVV_minus_{i}\" for i in range(1, 5)])\n",
    "]\n",
    "\n",
    "for name, feat_list in sar_experiments:\n",
    "    print(f\"\\nüöÄ Starte Training f√ºr Experiment: {name}\")\n",
    "    \n",
    "    # Trainieren (Nutze dein vorhandenes train_and_save_model)\n",
    "    # Falls die Funktion das Modell zur√ºckgibt, kannst du es hier speichern\n",
    "    train_and_save_model(\n",
    "        df_train=df_train_final, \n",
    "        df_val=df_val_final, \n",
    "        features=feat_list, \n",
    "        target_col=\"target_strict\", \n",
    "        model_name=f\"Model_Strict_{name}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "## Define feature combinations for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- BASIS-BAUSTEINE ---\n",
    "# Struktur & Kategorie (Immer dabei)\n",
    "base_structural = ['vh', 'vv', 'COP_DEM', 'lc_class']\n",
    "\n",
    "# Spektrale Historie (Lags)\n",
    "# Wir nehmen NDVI_t und die Lags t-1 bis t-2\n",
    "ndvi_basic_core = [\"NDVI_basic_t\"] + [f\"NDVI_basic_t_minus_{i}\" for i in range(1, 3)]\n",
    "ndvi_strict_core = [\"NDVI_strict_t\"] + [f\"NDVI_strict_t_minus_{i}\" for i in range(1, 3)]\n",
    "\n",
    "# Klima-Bl√∂cke (ERA5)\n",
    "temp_features = ['t2m_mean', 't2mmax_mean', 't2mmin_mean'] \n",
    "temp_features_with_lags = temp_features + [f\"{f}_minus_{i}\" for f in temp_features for i in range(1, 5)]\n",
    "precip_features = ['tp_dailymean_mean', 'tp_rollingmax_mean', 'tp_dailymax_mean', 'pei_30_mean', 'pei_90_mean']\n",
    "precip_features_with_lags = precip_features + [f\"{f}_minus_{i}\" for f in precip_features for i in range(1, 5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bausteine\n",
    "base_vars = ['COP_DEM', 'lc_class'] #+ ndvi_basic_core\n",
    "sar_indices = ['DpRVIVV', 'VHVVR', 'VVVHS']\n",
    "\n",
    "# Kombination A: Nur Rohdaten\n",
    "features_raw = base_vars + ['vv', 'vh']\n",
    "\n",
    "# Kombination B: Nur Indizes\n",
    "features_indices = base_vars + sar_indices\n",
    "\n",
    "# Kombination C: Beides (um zu sehen, ob sie sich erg√§nzen)\n",
    "features_combined = base_vars + ['vv', 'vh'] + sar_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPERIMENT 1: Baseline (Nur Struktur + Spektral)\n",
    "features_baseline_sar = base_structural \n",
    "\n",
    "\n",
    "features_baseline_spectral =  ['COP_DEM', 'lc_class', \"NDVI_basic_t\"]\n",
    "\n",
    "\n",
    "# EXPERIMENT 2: Baseline + Temperatur\n",
    "features_temp = base_structural + ndvi_basic_core + temp_features\n",
    "features_temp_plus_lags = base_structural + ndvi_basic_core + temp_features_with_lags\n",
    "\n",
    "# EXPERIMENT 3: Baseline + Wasser/Niederschlag\n",
    "features_water = base_structural + ndvi_basic_core + precip_features\n",
    "features_water_plus_lags = base_structural + ndvi_basic_core + precip_features_with_lags\n",
    "\n",
    "# EXPERIMENT 4: Full Environmental (Alles zusammen)\n",
    "features_full = base_structural + ndvi_basic_core + temp_features + precip_features\n",
    "\n",
    "features_full_with_lags = base_structural + ndvi_basic_core + temp_features_with_lags + precip_features_with_lags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liste der Experimente zum Durchlaufen\n",
    "experiments = [\n",
    "    # (\"Baseline SAR\", features_baseline_sar),\n",
    "    # (\"Baseline Spectral\", features_baseline_spectral),\n",
    "    # (\"Temp_Only\", features_temp),\n",
    "    # (\"Temp_Only_with_Lags\", features_temp_plus_lags),\n",
    "    (\"Water_Only\", features_water),\n",
    "    (\"Water_Only_with_Lags\", features_water_plus_lags),\n",
    "    (\"Full_Climate\", features_full),\n",
    "    (\"Full_Climate_with_Lags\", features_full_with_lags),\n",
    "    (\"SAR_Indices\", features_indices),\n",
    "    (\"Combined_SAR\", features_combined)\n",
    "]\n",
    "\n",
    "for name, feat_list in experiments:\n",
    "    print(f\"\\nüöÄ Starte Training f√ºr Experiment: {name}\")\n",
    "    \n",
    "    # Trainieren (Nutze dein vorhandenes train_and_save_model)\n",
    "    # Falls die Funktion das Modell zur√ºckgibt, kannst du es hier speichern\n",
    "    train_and_save_model(\n",
    "        df_train=df_train_final, \n",
    "        df_val=df_val_final, \n",
    "        features=feat_list, \n",
    "        target_col=\"target_basic\", \n",
    "        model_name=f\"Model_Basic_{name}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "## Create test set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sampled_master_test_df(cubes, test_keys, features, n_per_lc=2000):\n",
    "    \"\"\"\n",
    "    Erstellt ein Master-Test-Set aus mehreren Cubes.\n",
    "    Sorgt f√ºr eindeutige pixel_group IDs √ºber alle Cubes hinweg.\n",
    "    \"\"\"\n",
    "    print(f\"Erstelle Master-Test-Set aus Cubes: {test_keys}...\")\n",
    "    \n",
    "    all_samples = []\n",
    "    global_pixel_offset = 0\n",
    "\n",
    "    for key in test_keys:\n",
    "        print(f\"  Verarbeite Cube {key}...\")\n",
    "        ds_test = cubes[key]\n",
    "        n_t = ds_test.sizes[\"time_sentinel_2_l2a\"]\n",
    "\n",
    "        # DEM Vorbereitung\n",
    "        dem_full = ds_test[\"COP_DEM\"]\n",
    "        if \"time_cop_dem_glo_30_dged_cog\" in dem_full.dims:\n",
    "            dem_full = dem_full.mean(dim=\"time_cop_dem_glo_30_dged_cog\", skipna=True)\n",
    "\n",
    "        for lc_val in VEGETATION_CLASSES:\n",
    "            # Koordinaten finden\n",
    "            coords = np.argwhere((ds_test.ESA_LC.isel(time_esa_worldcover=0) == lc_val).values)\n",
    "            if len(coords) == 0: continue\n",
    "\n",
    "            n_draw = min(len(coords), n_per_lc)\n",
    "            idx = np.random.choice(len(coords), n_draw, replace=False)\n",
    "            selected_coords = coords[idx]\n",
    "\n",
    "            y_idx = xr.DataArray(selected_coords[:, 0], dims=\"pixel_id\")\n",
    "            x_idx = xr.DataArray(selected_coords[:, 1], dims=\"pixel_id\")\n",
    "            \n",
    "            pixel_bundle = ds_test.isel(y=y_idx, x=x_idx)\n",
    "            stacked = pixel_bundle.stack(sample=(\"pixel_id\", \"time_sentinel_2_l2a\"))\n",
    "\n",
    "            idx_t, idx_target = [], []\n",
    "            for p in range(n_draw):\n",
    "                start = p * n_t\n",
    "                idx_t.extend(range(start, start + n_t - 1))\n",
    "                idx_target.extend(range(start + 1, start + n_t))\n",
    "\n",
    "            # Eindeutige IDs: Offset vom vorherigen Cube/Klasse mitnehmen\n",
    "            unique_pixel_ids = np.arange(global_pixel_offset, global_pixel_offset + n_draw, dtype=np.uint32)\n",
    "\n",
    "            temp_dict = {f: stacked[f].isel(sample=idx_t).values.astype(np.float32) for f in features}\n",
    "            \n",
    "            temp_dict.update({\n",
    "                \"NDVI_basic_t\": stacked[\"NDVI_basic\"].isel(sample=idx_t).values.astype(np.float32),\n",
    "                \"target_basic\": stacked[\"NDVI_basic\"].isel(sample=idx_target).values.astype(np.float32),\n",
    "                \"NDVI_strict_t\": stacked[\"NDVI_strict\"].isel(sample=idx_t).values.astype(np.float32),\n",
    "                \"target_strict\": stacked[\"NDVI_strict\"].isel(sample=idx_target).values.astype(np.float32),\n",
    "                \"pixel_group\": np.repeat(unique_pixel_ids, n_t - 1),\n",
    "                \"timestep\": np.tile(np.arange(n_t - 1, dtype=np.uint16), n_draw),\n",
    "                \"lc_class\": lc_val,\n",
    "                \"cube_origin\": key, # Herkunft tracken\n",
    "                \"COP_DEM\": np.repeat(dem_full.isel(y=y_idx, x=x_idx).values.astype(np.float32), n_t - 1)\n",
    "            })\n",
    "            \n",
    "            all_samples.append(pd.DataFrame(temp_dict))\n",
    "            global_pixel_offset += n_draw # Offset erh√∂hen f√ºr den n√§chsten Durchlauf\n",
    "            \n",
    "            del stacked, pixel_bundle\n",
    "            gc.collect()\n",
    "\n",
    "    # Finaler Merge aller Cubes\n",
    "    df_master = pd.concat(all_samples, ignore_index=True)\n",
    "    df_master[\"lc_class\"] = df_master[\"lc_class\"].astype(\"category\")\n",
    "    \n",
    "    # Lags berechnen (jetzt √ºber alle Cubes hinweg sicher, da pixel_group eindeutig)\n",
    "    print(\"Berechne Lags f√ºr Master-Set...\")\n",
    "    for f in features + [\"NDVI_basic_t\", \"NDVI_strict_t\"]:\n",
    "        for lag in range(1, 6):\n",
    "            df_master[f\"{f}_minus_{lag}\"] = df_master.groupby(\"pixel_group\")[f].shift(lag)\n",
    "            \n",
    "    return df_master\n",
    "\n",
    "# --- Ausf√ºhrung ---\n",
    "test_keys = [\"ds_5\", \"ds_10\"]\n",
    "df_test_master = create_sampled_master_test_df(cubes, test_keys, features)\n",
    "\n",
    "# Speichern (z.B. mit Hinweis auf beide Cubes im Namen)\n",
    "df_test_master.to_parquet(f\"XGBoost/new_data/master_test_combined.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
