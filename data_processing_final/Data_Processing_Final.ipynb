{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Neue imports\n",
    "import os\n",
    "import s3fs\n",
    "import fsspec\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from scripts.sentinel_1_processing import find_global_veg_clipping_values, clip_s1_data, fast_lee_filter_optimized, apply_lee_to_ds, normalize_s1_vars, calculate_SAR_index #, aggregate_s1_causal_nearest \n",
    "from scripts.sentinel_2_processing import get_s2_quality_masks, get_vegetation_mask, clean_and_normalize_bands, calculate_s2_index, filter_static_vegetation_outliers, integrate_veg_and_wrongly_classified_mask\n",
    "from scripts.plot_helpers_new import plot_acquisition_timelines, find_cloud_free_indices, plot_rgb, plot_statistical_outliers\n",
    "from scripts.era_5_processing import subset_era5_spatial, subset_era5_time, aggregate_era5_metrics, create_uniform_era5_features, verify_era5_alignment \n",
    "from scripts.cube_processing  import add_event_metadata\n",
    "from scripts.aggregation_5_day_interval import  get_aggregation_information, align_all_to_5d, plot_acquisition_and_bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "S3_BASE_URL = \"https://s3.waw3-2.cloudferro.com/swift/v1/\"\n",
    "fs = s3fs.S3FileSystem(\n",
    "    anon=True,\n",
    "    client_kwargs={'endpoint_url': 'https://s3.waw3-2.cloudferro.com'}\n",
    ")\n",
    "\n",
    "#  Define Bucket_Name \n",
    "BUCKET_NAME = 'ARCEME-DC-1' # old bucket: \"ARCEME-DATACUBES/FIRSTBATCH\"\n",
    "\n",
    "# List all files (recursively) in the ARCEME-DC-1 bucket\n",
    "files = fs.ls(BUCKET_NAME, detail=True)\n",
    "zarr_files = [f['Key'] for f in files if f['Key'].endswith('.zarr')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Get additional data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = pd.read_csv(\"data/1_max_precipitation_grid_cells.csv\", sep = \",\")\n",
    "# info_data = pd.read_csv(\"data/consolidated_s2_lc_analysis.csv\", sep=\";\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## Load random cubes and add metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cubes = {}\n",
    "\n",
    "for i in range(1, 12): # 1 bis 4\n",
    "    random.seed(300000000+i)\n",
    "    nn = random.randrange(20, len(zarr_files))\n",
    "\n",
    "    file_path = zarr_files[nn]\n",
    "    url = S3_BASE_URL + zarr_files[nn]\n",
    "\n",
    "    cube_id = file_path.split('/DC__')[-1].replace('.zarr', '')\n",
    "    print(cube_id)\n",
    "    \n",
    "    # Mapper erstellen und Dataset öffnen\n",
    "    mapper = fsspec.get_mapper(url)\n",
    "    # Nutze 'chunks' oder 'consolidated=True' falls vorhanden für Speed\n",
    "    ds = xr.open_zarr(mapper, consolidated=True) \n",
    "\n",
    "    # Add cube_id to attrs\n",
    "    ds.attrs['cube_id'] = cube_id\n",
    "\n",
    "    # Get metadata (precip_dates, etc.)\n",
    "    ds = add_event_metadata(ds, df_data,cube_id)\n",
    "\n",
    "    # Im Dictionary speichern\n",
    "    cubes[f\"ds_{i}\"] = ds\n",
    "    print(f\"Cube {i} geladen: {zarr_files[nn]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## Examine Cubes\n",
    "## ADAPT PLOTTING FUNCTIONS TO LOGIC!!"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7",
   "metadata": {},
   "source": [
    "cubes_test = {}\n",
    "for i in cubes:\n",
    "    # cubes_test[i] = generate_quality_masks(cubes[i])\n",
    "    ind = find_cloud_free_indices(cubes[i])\n",
    "    plot_rgb(cubes[i],ind[0],cloud_comp=True)\n",
    "    plot_landcover(cubes[i])\n",
    "    result = landcover_distribution(cubes[i], LC_MAPPING)\n",
    "    for name, frac in result:\n",
    "        # Output format: Class Name: 0.XXXX\n",
    "        print(f\"{name}: {frac}\")\n",
    "\n",
    "    print(\"############################################################\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## Sentinel 2 processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in cubes.keys():\n",
    "    print(key)\n",
    "    ds = cubes[key]\n",
    "    ds = get_s2_quality_masks(ds)\n",
    "    ds = get_vegetation_mask(ds)\n",
    "    ds = clean_and_normalize_bands(ds)\n",
    "    ds = calculate_s2_index(ds, \"NDVI\", \"strict\")\n",
    "    ds = calculate_s2_index(ds, \"NDVI\", \"basic\")\n",
    "    ds = filter_static_vegetation_outliers(ds, \"NDVI_strict\")\n",
    "    ds = filter_static_vegetation_outliers(ds, \"NDVI_basic\")\n",
    "    indices = find_cloud_free_indices(ds)\n",
    "    if len(indices) > 0:\n",
    "        plot_statistical_outliers(ds, \"NDVI_basic\", indices[0])\n",
    "    ds =  integrate_veg_and_wrongly_classified_mask(ds, \"NDVI_basic\")\n",
    "    cubes[key] = ds.drop_vars([\"NDVI_strict_valid_mask_static\", \"mask_phys_basic\", \"mask_phys_strict\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## Sentinel 1 processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize S1 and S2 acquisition time steps\n",
    "for key in cubes.keys():\n",
    "    print(\"##############################\" , key, \"###################\")\n",
    "    plot_acquisition_timelines(cubes[key]) # Nutze hier deine Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get global 99.5 percentile values for vv and vh \n",
    "global_vv_max, global_vh_max = find_global_veg_clipping_values(cubes)\n",
    "print(global_vv_max, global_vh_max)\n",
    "\n",
    "for key in cubes.keys():\n",
    "    print(key)\n",
    "    \n",
    "    # Apply clipping\n",
    "    cubes[key] = clip_s1_data(cubes[key], global_vv_max, global_vh_max)\n",
    "    \n",
    "    # Apply lee filtering\n",
    "    cubes[key] = apply_lee_to_ds(cubes[key], bands=['vv', 'vh'], win_size=7, cu=0.25)\n",
    "\n",
    "    # Normalize bands\n",
    "    cubes[key] = normalize_s1_vars(cubes[key], global_vv_max, global_vh_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in cubes.keys():\n",
    "    print(f\"################################ Cube: {key} ################################\")\n",
    "    cubes[key] = align_all_to_5d(cubes[key], \"basic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## Optional (but maybe worth testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in cubes.keys():\n",
    "    for index in [\"DpRVIVV\", \"VVVHS\", \"VHVVR\"]:\n",
    "        print(f\"Calculating index: {index}\")\n",
    "        cubes[key][index] = calculate_SAR_index(cubes[key], index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "# ERA-5 climate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/net/data/arceme/era5_land/\"\n",
    "pei_cube_name = \"PEICube_era5land.zarr\"\n",
    "t2_cube_name = \"t2_ERA5land.zarr\"\n",
    "tp_cube_name = \"tp_ERA5land.zarr\"\n",
    "\n",
    "pei_cube = xr.open_zarr(os.path.join(path, pei_cube_name), consolidated=False)\n",
    "t2_cube  = xr.open_zarr(os.path.join(path, t2_cube_name), consolidated=False)\n",
    "tp_cube  = xr.open_zarr(os.path.join(path, tp_cube_name), consolidated=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "era5_cubes = [pei_cube, t2_cube, tp_cube]\n",
    "\n",
    "for key in cubes.keys():\n",
    "    ds = cubes[key]\n",
    "    era5_features = [] # Liste für gesammelte ERA5-Cubes\n",
    "\n",
    "    for era5_cube_raw in era5_cubes:\n",
    "        # 1. Räumlich/Zeitlicher Subset\n",
    "        temp_cube = subset_era5_time(era5_cube_raw, ds)\n",
    "        temp_cube = subset_era5_spatial(temp_cube, ds, True)\n",
    "        \n",
    "        era5_vars = list(temp_cube.data_vars)\n",
    "        \n",
    "        # 2. Aggregation & Alignment\n",
    "        temp_cube = aggregate_era5_metrics_new(temp_cube, ds, era5_vars)\n",
    "        temp_cube = create_uniform_era5_features(ds, temp_cube)\n",
    "\n",
    "        # 3. Absicherung: Koordinaten exakt angleichen\n",
    "        temp_cube = temp_cube.reindex_like(ds, method='nearest')\n",
    "        \n",
    "        era5_features.append(temp_cube)\n",
    "\n",
    "    # 4. Alle ERA5 Variablen auf einmal zum S2-Cube hinzufügen\n",
    "    ds = xr.merge([ds] + era5_features)\n",
    "    cubes[key] = ds\n",
    "    print(f\"✅ Integrated all ERA5 variables into cube {key}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "## Now check for datatypes, variables to drop and dimensions and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop = ['B01',\n",
    " 'B02',\n",
    " 'B03',\n",
    " 'B04',\n",
    " 'B05',\n",
    " 'B06',\n",
    " 'B07',\n",
    " 'B08',\n",
    " 'B09',\n",
    " 'B11',\n",
    " 'B12',\n",
    " 'quality_mask_basic',\n",
    " 'quality_mask_strict',#\n",
    " 's1_quality_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "cubes[\"ds_1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in  cubes.keys():\n",
    "    # cubes[key] = cubes[key].drop_vars(to_drop)\n",
    "    cubes[key][\"vv\"] = cubes[key][\"vv\"].astype(\"float32\")\n",
    "    cubes[key][\"vh\"] = cubes[key][\"vh\"].astype(\"float32\")\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Zielverzeichnis erstellen\n",
    "output_dir = \"../processed_data_final_new\"\n",
    "\n",
    "for key, ds in cubes.items():\n",
    "    # Definiere den Pfad für diesen spezifischen Cube innerhalb des Zarr-Groups\n",
    "    # oder als separaten Store\n",
    "    cube_path = f\"{output_dir}/{key}\"\n",
    "    \n",
    "    # Effizientes Speichern:\n",
    "    # 1. 'compute=True' führt die Dask-Berechnungen jetzt aus\n",
    "    # 2. 'mode=w' schreibt den Cube (oder 'a' für anfügen)\n",
    "    ds.to_zarr(cube_path, mode='w', consolidated=True)\n",
    "    print(f\"✅ Cube {key} successfully saved to Zarr as float32.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
